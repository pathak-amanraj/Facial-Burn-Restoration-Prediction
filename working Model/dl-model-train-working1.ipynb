{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13806440,"sourceType":"datasetVersion","datasetId":8791157}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 1: Install Dependencies (Minimal - Pure PyTorch)\n# =============================================================================\n# Fix numpy version conflict first\n!pip install -q numpy==1.24.3 --force-reinstall\n!pip install -q pillow tqdm matplotlib\n\nprint(\"‚úÖ Dependencies installed!\")\nprint(\"‚ö†Ô∏è  RESTART THE KERNEL NOW: Runtime ‚Üí Restart session\")\nprint(\"   Then SKIP this cell and run Cell 2\")","metadata":{"_uuid":"fb3ab34e-74b8-4d95-87a7-98bb34c543b2","_cell_guid":"2b186cb9-ccc5-4917-8c2c-64f94b758461","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T20:42:04.689037Z","iopub.execute_input":"2025-11-20T20:42:04.689722Z","iopub.status.idle":"2025-11-20T20:42:13.613061Z","shell.execute_reply.started":"2025-11-20T20:42:04.689692Z","shell.execute_reply":"2025-11-20T20:42:13.612185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2: Imports\n# =============================================================================\nimport os\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport gc\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torchvision.models as models\n\n# Set seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"‚úÖ Using device: {DEVICE}\")\nif DEVICE == 'cuda':\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")","metadata":{"_uuid":"b3fcd7f1-3a15-41d0-bec4-476a741f4d80","_cell_guid":"41d4c8a4-8fd0-4089-9d04-362d6f297fca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T20:42:30.421372Z","iopub.execute_input":"2025-11-20T20:42:30.422173Z","iopub.status.idle":"2025-11-20T20:42:33.274199Z","shell.execute_reply.started":"2025-11-20T20:42:30.422134Z","shell.execute_reply":"2025-11-20T20:42:33.273409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 3: Configuration\n# =============================================================================\n# Paths\nDATASET_PATH = '/kaggle/input/burn-scar-faces-with-original'\nCLEAN_FOLDER = os.path.join(DATASET_PATH, 'original')\nBURNS_FOLDER = os.path.join(DATASET_PATH, 'with_burns')\nOUTPUT_DIR = '/kaggle/working/outputs'\nCHECKPOINT_DIR = '/kaggle/working/checkpoints'\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# Training settings\nIMAGE_SIZE = 256          # Good balance of quality and speed\nBATCH_SIZE = 16           # Increased for faster training with full dataset\nMAX_IMAGES = None         # None = use ALL images (7000)\nEPOCHS = 30               # Reduced epochs since we have more data\nLR_G = 2e-4               # Generator learning rate\nLR_D = 2e-4               # Discriminator learning rate\nLAMBDA_L1 = 100           # Weight for L1 loss\nLAMBDA_PERCEPTUAL = 10    # Weight for perceptual loss\nSAVE_EVERY = 5            # Save checkpoint every N epochs (more frequent)\n\nprint(f\"‚úÖ Config loaded\")\nprint(f\"   Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\nprint(f\"   Batch size: {BATCH_SIZE}\")\nprint(f\"   Training on {MAX_IMAGES} images for {EPOCHS} epochs\")","metadata":{"_uuid":"8821ec04-49a0-4774-b4f1-b42cbef1a1e4","_cell_guid":"c2c5188b-791e-4012-a087-f437c98ff039","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T21:15:31.966535Z","iopub.execute_input":"2025-11-20T21:15:31.967276Z","iopub.status.idle":"2025-11-20T21:15:31.973637Z","shell.execute_reply.started":"2025-11-20T21:15:31.967254Z","shell.execute_reply":"2025-11-20T21:15:31.972807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# CELL 4: Dataset Class\n# =============================================================================\nclass BurnFaceDataset(Dataset):\n    \"\"\"\n    Paired dataset: burned face ‚Üí clean face\n    \"\"\"\n    def __init__(self, clean_folder, burns_folder, max_images=None, image_size=256, augment=True):\n        self.image_size = image_size\n        self.augment = augment\n        self.pairs = []\n        \n        # Get sorted file lists\n        clean_files = sorted([f for f in os.listdir(clean_folder) \n                             if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n        burn_files = sorted([f for f in os.listdir(burns_folder) \n                            if f.lower().endswith(('.jpg', '.png', '.jpeg'))])\n        \n        # Pair by index\n        num_pairs = min(len(clean_files), len(burn_files))\n        if max_images:\n            num_pairs = min(num_pairs, max_images)\n        \n        for i in range(num_pairs):\n            self.pairs.append((\n                os.path.join(burns_folder, burn_files[i]),   # Input: burned\n                os.path.join(clean_folder, clean_files[i])   # Target: clean\n            ))\n        \n        # Base transforms\n        self.to_tensor = transforms.ToTensor()\n        self.normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        \n        print(f\"‚úÖ Dataset loaded: {len(self.pairs)} image pairs\")\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        burn_path, clean_path = self.pairs[idx]\n        \n        # Load images\n        burn_img = Image.open(burn_path).convert('RGB')\n        clean_img = Image.open(clean_path).convert('RGB')\n        \n        # Resize\n        burn_img = burn_img.resize((self.image_size, self.image_size), Image.BILINEAR)\n        clean_img = clean_img.resize((self.image_size, self.image_size), Image.BILINEAR)\n        \n        # Data augmentation (same transform for both)\n        if self.augment and random.random() > 0.5:\n            burn_img = burn_img.transpose(Image.FLIP_LEFT_RIGHT)\n            clean_img = clean_img.transpose(Image.FLIP_LEFT_RIGHT)\n        \n        # To tensor and normalize to [-1, 1]\n        burn_tensor = self.normalize(self.to_tensor(burn_img))\n        clean_tensor = self.normalize(self.to_tensor(clean_img))\n        \n        return {\n            'burned': burn_tensor,\n            'clean': clean_tensor,\n            'burn_path': burn_path\n        }\n\n# Test dataset\ndataset = BurnFaceDataset(CLEAN_FOLDER, BURNS_FOLDER, max_images=MAX_IMAGES, image_size=IMAGE_SIZE)\nprint(f\"   Sample pair loaded successfully!\")","metadata":{"_uuid":"06d5eab7-e911-42f7-9300-58c0be8a55fd","_cell_guid":"59ccba83-318f-46a8-82ec-30ad59976fe5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T21:15:51.201475Z","iopub.execute_input":"2025-11-20T21:15:51.201763Z","iopub.status.idle":"2025-11-20T21:15:51.222919Z","shell.execute_reply.started":"2025-11-20T21:15:51.201743Z","shell.execute_reply":"2025-11-20T21:15:51.222111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 5: U-Net Generator Architecture\n# =============================================================================\nclass ConvBlock(nn.Module):\n    \"\"\"Encoder block: Conv -> BatchNorm -> LeakyReLU\"\"\"\n    def __init__(self, in_ch, out_ch, use_bn=True, use_dropout=False):\n        super().__init__()\n        layers = [nn.Conv2d(in_ch, out_ch, 4, stride=2, padding=1, bias=False)]\n        if use_bn:\n            layers.append(nn.BatchNorm2d(out_ch))\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n        if use_dropout:\n            layers.append(nn.Dropout(0.5))\n        self.block = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.block(x)\n\nclass DeconvBlock(nn.Module):\n    \"\"\"Decoder block: ConvTranspose -> BatchNorm -> ReLU -> (Dropout)\"\"\"\n    def __init__(self, in_ch, out_ch, use_dropout=False):\n        super().__init__()\n        layers = [\n            nn.ConvTranspose2d(in_ch, out_ch, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        ]\n        if use_dropout:\n            layers.append(nn.Dropout(0.5))\n        self.block = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.block(x)\n\nclass UNetGenerator(nn.Module):\n    \"\"\"\n    U-Net Generator for image-to-image translation\n    Input: burned face (3 channels)\n    Output: predicted clean face (3 channels)\n    \"\"\"\n    def __init__(self, in_channels=3, out_channels=3, features=64):\n        super().__init__()\n        \n        # Encoder (downsampling)\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(in_channels, features, 4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True)\n        )  # 256 -> 128\n        self.enc2 = ConvBlock(features, features*2)      # 128 -> 64\n        self.enc3 = ConvBlock(features*2, features*4)    # 64 -> 32\n        self.enc4 = ConvBlock(features*4, features*8)    # 32 -> 16\n        self.enc5 = ConvBlock(features*8, features*8)    # 16 -> 8\n        self.enc6 = ConvBlock(features*8, features*8)    # 8 -> 4\n        self.enc7 = ConvBlock(features*8, features*8)    # 4 -> 2\n        \n        # Bottleneck\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(features*8, features*8, 4, stride=2, padding=1),  # 2 -> 1\n            nn.ReLU(inplace=True)\n        )\n        \n        # Decoder (upsampling with skip connections)\n        self.dec1 = DeconvBlock(features*8, features*8, use_dropout=True)    # 1 -> 2\n        self.dec2 = DeconvBlock(features*8*2, features*8, use_dropout=True)  # 2 -> 4\n        self.dec3 = DeconvBlock(features*8*2, features*8, use_dropout=True)  # 4 -> 8\n        self.dec4 = DeconvBlock(features*8*2, features*8)   # 8 -> 16\n        self.dec5 = DeconvBlock(features*8*2, features*4)   # 16 -> 32\n        self.dec6 = DeconvBlock(features*4*2, features*2)   # 32 -> 64\n        self.dec7 = DeconvBlock(features*2*2, features)     # 64 -> 128\n        \n        # Final layer\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(features*2, out_channels, 4, stride=2, padding=1),  # 128 -> 256\n            nn.Tanh()\n        )\n    \n    def forward(self, x):\n        # Encoder\n        e1 = self.enc1(x)\n        e2 = self.enc2(e1)\n        e3 = self.enc3(e2)\n        e4 = self.enc4(e3)\n        e5 = self.enc5(e4)\n        e6 = self.enc6(e5)\n        e7 = self.enc7(e6)\n        \n        # Bottleneck\n        b = self.bottleneck(e7)\n        \n        # Decoder with skip connections\n        d1 = self.dec1(b)\n        d1 = torch.cat([d1, e7], dim=1)\n        \n        d2 = self.dec2(d1)\n        d2 = torch.cat([d2, e6], dim=1)\n        \n        d3 = self.dec3(d2)\n        d3 = torch.cat([d3, e5], dim=1)\n        \n        d4 = self.dec4(d3)\n        d4 = torch.cat([d4, e4], dim=1)\n        \n        d5 = self.dec5(d4)\n        d5 = torch.cat([d5, e3], dim=1)\n        \n        d6 = self.dec6(d5)\n        d6 = torch.cat([d6, e2], dim=1)\n        \n        d7 = self.dec7(d6)\n        d7 = torch.cat([d7, e1], dim=1)\n        \n        return self.final(d7)\n\nprint(\"‚úÖ U-Net Generator defined\")\n","metadata":{"_uuid":"b548b83e-ed3d-454f-8323-118a1832cc89","_cell_guid":"dd18d415-653b-46fd-bbfc-d76a3600876f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T21:16:04.093491Z","iopub.execute_input":"2025-11-20T21:16:04.093785Z","iopub.status.idle":"2025-11-20T21:16:04.108330Z","shell.execute_reply.started":"2025-11-20T21:16:04.093763Z","shell.execute_reply":"2025-11-20T21:16:04.107485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# CELL 6: PatchGAN Discriminator\n# =============================================================================\nclass PatchDiscriminator(nn.Module):\n    \"\"\"\n    PatchGAN Discriminator - classifies 70x70 patches as real/fake\n    Input: concatenated burned + clean/generated images (6 channels)\n    \"\"\"\n    def __init__(self, in_channels=6, features=64):\n        super().__init__()\n        \n        self.model = nn.Sequential(\n            # Layer 1: No BatchNorm\n            nn.Conv2d(in_channels, features, 4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Layer 2\n            nn.Conv2d(features, features*2, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features*2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Layer 3\n            nn.Conv2d(features*2, features*4, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features*4),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Layer 4\n            nn.Conv2d(features*4, features*8, 4, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(features*8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Output layer\n            nn.Conv2d(features*8, 1, 4, stride=1, padding=1)\n        )\n    \n    def forward(self, x, y):\n        # Concatenate input and target/generated\n        combined = torch.cat([x, y], dim=1)\n        return self.model(combined)\n\nprint(\"‚úÖ PatchGAN Discriminator defined\")","metadata":{"_uuid":"2d99b88c-c438-4745-b06a-f9879b694e02","_cell_guid":"bad8509a-02db-4b4a-96dc-4f4c0cc1239d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T21:16:07.777448Z","iopub.execute_input":"2025-11-20T21:16:07.777723Z","iopub.status.idle":"2025-11-20T21:16:07.784970Z","shell.execute_reply.started":"2025-11-20T21:16:07.777703Z","shell.execute_reply":"2025-11-20T21:16:07.784097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# CELL 7: Perceptual Loss (VGG-based)\n# =============================================================================\nclass VGGPerceptualLoss(nn.Module):\n    \"\"\"Perceptual loss using VGG16 features\"\"\"\n    def __init__(self):\n        super().__init__()\n        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        self.features = nn.Sequential(*list(vgg.features[:16])).eval()\n        for param in self.features.parameters():\n            param.requires_grad = False\n        \n        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n    \n    def forward(self, pred, target):\n        # Normalize from [-1,1] to VGG range\n        pred = (pred + 1) / 2\n        target = (target + 1) / 2\n        \n        pred = (pred - self.mean) / self.std\n        target = (target - self.mean) / self.std\n        \n        return F.l1_loss(self.features(pred), self.features(target))\n\nprint(\"‚úÖ Perceptual Loss defined\")","metadata":{"_uuid":"92e1de01-f8d4-41bd-bdc6-b67fc795a86a","_cell_guid":"6b5bbf5a-50e5-41f4-90ca-7611ecb57eba","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T21:16:14.813486Z","iopub.execute_input":"2025-11-20T21:16:14.813769Z","iopub.status.idle":"2025-11-20T21:16:14.820392Z","shell.execute_reply.started":"2025-11-20T21:16:14.813749Z","shell.execute_reply":"2025-11-20T21:16:14.819688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# CELL 8: Initialize Models and Training\n# =============================================================================\n# Initialize models\ngenerator = UNetGenerator().to(DEVICE)\ndiscriminator = PatchDiscriminator().to(DEVICE)\nvgg_loss = VGGPerceptualLoss().to(DEVICE)\n\n# Count parameters\ng_params = sum(p.numel() for p in generator.parameters())\nd_params = sum(p.numel() for p in discriminator.parameters())\nprint(f\"‚úÖ Generator parameters: {g_params:,}\")\nprint(f\"‚úÖ Discriminator parameters: {d_params:,}\")\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=LR_G, betas=(0.5, 0.999))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR_D, betas=(0.5, 0.999))\n\n# Loss functions\ncriterion_GAN = nn.BCEWithLogitsLoss()\ncriterion_L1 = nn.L1Loss()\n\n# DataLoader\ndataloader = DataLoader(\n    dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"‚úÖ Training setup complete!\")\nprint(f\"   Steps per epoch: {len(dataloader)}\")","metadata":{"_uuid":"d7196a73-2787-4d81-9924-6c1bc01a6b75","_cell_guid":"a34946fb-e93f-41e1-a583-f9916c3e9d3c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T21:16:19.828331Z","iopub.execute_input":"2025-11-20T21:16:19.828610Z","iopub.status.idle":"2025-11-20T21:16:22.087714Z","shell.execute_reply.started":"2025-11-20T21:16:19.828591Z","shell.execute_reply":"2025-11-20T21:16:22.086956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# CELL 9: Training Loop\n# =============================================================================\ndef train():\n    print(\"\\n\" + \"=\"*60)\n    print(\"üöÄ STARTING TRAINING\")\n    print(\"=\"*60 + \"\\n\")\n    \n    history = {'g_loss': [], 'd_loss': []}\n    \n    for epoch in range(EPOCHS):\n        generator.train()\n        discriminator.train()\n        \n        epoch_g_loss = 0\n        epoch_d_loss = 0\n        \n        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n        \n        for batch in pbar:\n            burned = batch['burned'].to(DEVICE)\n            clean = batch['clean'].to(DEVICE)\n            batch_size = burned.size(0)\n            \n            # Labels\n            real_label = torch.ones(batch_size, 1, 30, 30).to(DEVICE)\n            fake_label = torch.zeros(batch_size, 1, 30, 30).to(DEVICE)\n            \n            # ==================\n            # Train Discriminator\n            # ==================\n            optimizer_D.zero_grad()\n            \n            # Real loss\n            pred_real = discriminator(burned, clean)\n            loss_D_real = criterion_GAN(pred_real, real_label)\n            \n            # Fake loss\n            fake_clean = generator(burned)\n            pred_fake = discriminator(burned, fake_clean.detach())\n            loss_D_fake = criterion_GAN(pred_fake, fake_label)\n            \n            # Combined D loss\n            loss_D = (loss_D_real + loss_D_fake) * 0.5\n            loss_D.backward()\n            optimizer_D.step()\n            \n            # ==================\n            # Train Generator\n            # ==================\n            optimizer_G.zero_grad()\n            \n            # GAN loss\n            pred_fake = discriminator(burned, fake_clean)\n            loss_G_GAN = criterion_GAN(pred_fake, real_label)\n            \n            # L1 loss\n            loss_G_L1 = criterion_L1(fake_clean, clean)\n            \n            # Perceptual loss\n            loss_G_perceptual = vgg_loss(fake_clean, clean)\n            \n            # Combined G loss\n            loss_G = loss_G_GAN + LAMBDA_L1 * loss_G_L1 + LAMBDA_PERCEPTUAL * loss_G_perceptual\n            loss_G.backward()\n            optimizer_G.step()\n            \n            # Logging\n            epoch_g_loss += loss_G.item()\n            epoch_d_loss += loss_D.item()\n            \n            pbar.set_postfix({\n                'G': f'{loss_G.item():.3f}',\n                'D': f'{loss_D.item():.3f}',\n                'L1': f'{loss_G_L1.item():.3f}'\n            })\n        \n        # Epoch summary\n        avg_g_loss = epoch_g_loss / len(dataloader)\n        avg_d_loss = epoch_d_loss / len(dataloader)\n        history['g_loss'].append(avg_g_loss)\n        history['d_loss'].append(avg_d_loss)\n        \n        print(f\"üìä Epoch {epoch+1}: G_loss={avg_g_loss:.4f}, D_loss={avg_d_loss:.4f}\")\n        \n        # Save checkpoint and samples\n        if (epoch + 1) % SAVE_EVERY == 0 or epoch == EPOCHS - 1:\n            # Save model\n            torch.save({\n                'epoch': epoch,\n                'generator': generator.state_dict(),\n                'discriminator': discriminator.state_dict(),\n                'optimizer_G': optimizer_G.state_dict(),\n                'optimizer_D': optimizer_D.state_dict(),\n            }, os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch+1}.pt'))\n            print(f\"üíæ Saved checkpoint at epoch {epoch+1}\")\n            \n            # Save sample images\n            save_samples(epoch+1)\n        \n        # Clear cache\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    # Save final model\n    torch.save(generator.state_dict(), os.path.join(CHECKPOINT_DIR, 'generator_final.pt'))\n    print(f\"\\nüéâ Training complete! Final model saved.\")\n    \n    return history\n\ndef save_samples(epoch, num_samples=4):\n    \"\"\"Save sample predictions\"\"\"\n    generator.eval()\n    \n    fig, axes = plt.subplots(num_samples, 3, figsize=(12, num_samples*4))\n    \n    with torch.no_grad():\n        for i in range(num_samples):\n            idx = random.randint(0, len(dataset)-1)\n            sample = dataset[idx]\n            \n            burned = sample['burned'].unsqueeze(0).to(DEVICE)\n            clean = sample['clean'].unsqueeze(0).to(DEVICE)\n            \n            predicted = generator(burned)\n            \n            # Convert to numpy for display\n            burned_np = (burned[0].cpu().numpy().transpose(1, 2, 0) + 1) / 2\n            predicted_np = (predicted[0].cpu().numpy().transpose(1, 2, 0) + 1) / 2\n            clean_np = (clean[0].cpu().numpy().transpose(1, 2, 0) + 1) / 2\n            \n            # Clip values\n            burned_np = np.clip(burned_np, 0, 1)\n            predicted_np = np.clip(predicted_np, 0, 1)\n            clean_np = np.clip(clean_np, 0, 1)\n            \n            axes[i, 0].imshow(burned_np)\n            axes[i, 0].set_title('Input (Burned)')\n            axes[i, 0].axis('off')\n            \n            axes[i, 1].imshow(predicted_np)\n            axes[i, 1].set_title('Predicted (Post-Surgery)')\n            axes[i, 1].axis('off')\n            \n            axes[i, 2].imshow(clean_np)\n            axes[i, 2].set_title('Ground Truth (Clean)')\n            axes[i, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, f'samples_epoch_{epoch}.png'), dpi=150)\n    plt.close()\n    print(f\"üì∏ Saved samples for epoch {epoch}\")\n    \n    generator.train()\n\nprint(\"‚úÖ Training functions defined\")\nprint(\"\\n‚è≥ Run the next cell to start training...\")","metadata":{"_uuid":"3f338c78-a3e9-4909-b3db-92aba106470a","_cell_guid":"b91e0762-28e1-4ef5-ae62-c8b0306657be","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-20T21:16:41.591886Z","iopub.execute_input":"2025-11-20T21:16:41.592449Z","iopub.status.idle":"2025-11-20T21:16:41.607434Z","shell.execute_reply.started":"2025-11-20T21:16:41.592425Z","shell.execute_reply":"2025-11-20T21:16:41.606744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================================================\n# CELL 10: START TRAINING\n# =============================================================================\n# Run training\nhistory = train()\n\n# Plot training history\nplt.figure(figsize=(10, 5))\nplt.plot(history['g_loss'], label='Generator Loss')\nplt.plot(history['d_loss'], label='Discriminator Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training History')\nplt.legend()\nplt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T21:16:55.438852Z","iopub.execute_input":"2025-11-20T21:16:55.439635Z","iopub.status.idle":"2025-11-20T21:55:10.162165Z","shell.execute_reply.started":"2025-11-20T21:16:55.439607Z","shell.execute_reply":"2025-11-20T21:55:10.161361Z"}},"outputs":[],"execution_count":null}]}